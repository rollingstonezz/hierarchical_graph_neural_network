{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b18e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.5.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GINConv, GraphConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from itertools import combinations\n",
    "import time\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, ReLU\n",
    "import argparse\n",
    "import os, sys\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch.autograd import Variable\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from networkx.algorithms.components import strongly_connected_components\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, DataLoader\n",
    "from torch_geometric.nn import BatchNorm, SAGEConv\n",
    "import os.path as osp\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.datasets import TUDataset, MoleculeNet, GEDDataset\n",
    "from utils.GNNBenchmarkDataset import GNNBenchmarkDataset\n",
    "from utils.UPFD import UPFD\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, DataLoader, ShaDowKHopSampler\n",
    "from torch_geometric.nn import BatchNorm, SAGEConv\n",
    "from torch_geometric.utils import contains_isolated_nodes\n",
    "from torch_sparse import SparseTensor, cat\n",
    "from torch_scatter import scatter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from models.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8e7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_dir = '../../data'\n",
    "        self.save_dir = '../../data/processed'\n",
    "        self.write_dir = '.results/processed'\n",
    "        self.dataset = 'PROTEINS'\n",
    "        self.idx = 1\n",
    "        self.gpu = 0\n",
    "        self.num_clusters = 5\n",
    "        self.hidden_dim = 64\n",
    "        self.number_layers = 2\n",
    "        self.lr = 5e-4\n",
    "        \n",
    "args = Args()\n",
    "\n",
    "dataset_name = args.dataset\n",
    "DATA_PATH = args.data_dir\n",
    "idx = args.idx\n",
    "\n",
    "if dataset_name in ['DD', 'MUTAG', 'PROTEINS', 'IMDB-BINARY', 'IMDB-MULTI', 'REDDIT-BINARY', 'COLLAB']:\n",
    "    path = osp.join(DATA_PATH, 'TUDataset')\n",
    "    dataset = TUDataset(path, name=dataset_name)\n",
    "    num_splits = 10\n",
    "elif dataset_name in ['brain']:\n",
    "    path = osp.join(DATA_PATH, 'brain')\n",
    "    with open(os.path.join(path, 'sc.pkl'), 'rb') as f:\n",
    "        dataset = pkl.load(f)\n",
    "    num_splits = 10\n",
    "elif dataset_name in ['UPFD']:\n",
    "    path = osp.join(DATA_PATH, 'UPFD')\n",
    "    dataset_train = UPFD(path, feature='content', name='politifact', split='train')\n",
    "    dataset_val = UPFD(path, feature='content', name='politifact', split='val')\n",
    "    dataset_test = UPFD(path, feature='content', name='politifact', split='test')\n",
    "elif dataset_name in ['MNIST', 'CIFAR10']:\n",
    "    path = osp.join(DATA_PATH, 'GNNBenchmarkDataset')\n",
    "    dataset_train = GNNBenchmarkDataset(path, name=dataset_name, split='train')\n",
    "    dataset_val = GNNBenchmarkDataset(path, name=dataset_name, split='val')\n",
    "    dataset_test = GNNBenchmarkDataset(path, name=dataset_name, split='test')\n",
    "elif dataset_name in ['hiv', 'bace', 'bbbp']:\n",
    "    path = osp.join(DATA_PATH, 'PygGraphPropPredDataset')\n",
    "    dataset = PygGraphPropPredDataset(name = 'ogbg-mol'+dataset_name, root = path)   \n",
    "    split_indices = dataset.get_idx_split() \n",
    "    train_indices = split_indices[\"train\"]\n",
    "    val_indices = split_indices[\"valid\"]\n",
    "    test_indices = split_indices[\"test\"]\n",
    "    dataset_train = dataset[train_indices]\n",
    "    dataset_val = dataset[val_indices]\n",
    "    dataset_test = dataset[test_indices]\n",
    "if idx>1 and not dataset_name in ['DD', 'MUTAG', 'PROTEINS', 'IMDB-BINARY', 'IMDB-MULTI','COLLAB', 'brain', 'REDDIT-BINARY']:\n",
    "    raise ValueError(dataset_name + 'does not have 10-fold validation.')\n",
    "    exit()\n",
    "\n",
    "if dataset_name in ['MNIST', 'CIFAR10', 'hiv', 'bace', 'bbbp', 'UPFD']:\n",
    "\n",
    "    path = osp.join(args.save_dir, dataset_name)\n",
    "\n",
    "    assert os.path.exists(path)\n",
    "\n",
    "    with open(osp.join(path, 'batched_data_cluster'+str(args.num_clusters)+'.pkl'), 'rb') as f:\n",
    "        store_dataset = pkl.load(f)\n",
    "\n",
    "    batched_dataset_train_node = store_dataset['batched_dataset_train_node']\n",
    "    batched_dataset_train_edge = store_dataset['batched_dataset_train_edge']\n",
    "    clustered_edge_index_train = store_dataset['clustered_edge_index_train']\n",
    "    clustered_batch_train = store_dataset['clustered_batch_train']\n",
    "    y_true_train = store_dataset['y_true_train']\n",
    "    batched_dataset_val_node = store_dataset['batched_dataset_val_node']\n",
    "    batched_dataset_val_edge = store_dataset['batched_dataset_val_edge']\n",
    "    clustered_edge_index_val = store_dataset['clustered_edge_index_val']\n",
    "    clustered_batch_val = store_dataset['clustered_batch_val']\n",
    "    y_true_val = store_dataset['y_true_val']\n",
    "    batched_dataset_test_node = store_dataset['batched_dataset_test_node']\n",
    "    batched_dataset_test_edge = store_dataset['batched_dataset_test_edge']\n",
    "    clustered_edge_index_test = store_dataset['clustered_edge_index_test']\n",
    "    clustered_batch_test = store_dataset['clustered_batch_test']\n",
    "    y_true_test = store_dataset['y_true_test']\n",
    "elif dataset_name in ['DD', 'MUTAG', 'PROTEINS', 'IMDB-BINARY', 'IMDB-MULTI','COLLAB', 'brain', 'REDDIT-BINARY']:\n",
    "    i = idx\n",
    "    path = osp.join(args.save_dir, dataset_name)\n",
    "    with open(osp.join(path, 'batched_data_'+str(i)+'_cluster'+str(args.num_clusters)+'.pkl'), 'rb') as f:\n",
    "        store_dataset = pkl.load(f)\n",
    "\n",
    "    batched_dataset_train_node = store_dataset['batched_dataset_train_node']\n",
    "    batched_dataset_train_edge = store_dataset['batched_dataset_train_edge']\n",
    "    clustered_edge_index_train = store_dataset['clustered_edge_index_train']\n",
    "    clustered_batch_train = store_dataset['clustered_batch_train']\n",
    "    y_true_train = store_dataset['y_true_train']\n",
    "    batched_dataset_test_node = store_dataset['batched_dataset_test_node']\n",
    "    batched_dataset_test_edge = store_dataset['batched_dataset_test_edge']\n",
    "    clustered_edge_index_test = store_dataset['clustered_edge_index_test']\n",
    "    clustered_batch_test = store_dataset['clustered_batch_test']\n",
    "    y_true_test = store_dataset['y_true_test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54173c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROTEINS(1113)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55272e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset = DataLoader(dataset, batch_size=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d914066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in batched_dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f268b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import WLConv\n",
    "wlconv = WLConv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ace6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_x_first = wlconv(\n",
    "    torch.ones((data.num_nodes), dtype=torch.long), \n",
    "    data.edge_index\n",
    ")\n",
    "hashed_x_second = wlconv(\n",
    "    hashed_x_first, \n",
    "    data.edge_index\n",
    ")\n",
    "hist = wlconv.histogram(hashed_x_second, data.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e25de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_num_nodes = int(torch.max(clustered_edge_index_train[0])+1)\n",
    "coarse_hashed_x_first = wlconv(\n",
    "    torch.ones((coarse_num_nodes), dtype=torch.long), \n",
    "    clustered_edge_index_train[0]\n",
    ")\n",
    "coarse_hashed_x_second = wlconv(\n",
    "    coarse_hashed_x_first, \n",
    "    clustered_edge_index_train[0]\n",
    ")\n",
    "coarse_hist = wlconv.histogram(coarse_hashed_x_second, clustered_batch_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7aa4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_hist_normalized = hist.sum(axis=0)\n",
    "coarse_hist_normalized = coarse_hist.sum(axis=0)\n",
    "fine_hist_normalized = fine_hist_normalized / fine_hist_normalized.sum()\n",
    "coarse_hist_normalized = coarse_hist_normalized / coarse_hist_normalized.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fd2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_hist_normalized_np = fine_hist_normalized.numpy()\n",
    "coarse_hist_normalized_np = coarse_hist_normalized.numpy()\n",
    "longlength = max(len(fine_hist_normalized_np), len(coarse_hist_normalized_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a638942",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_hist_normalized_np = np.concatenate([fine_hist_normalized_np, np.zeros(longlength - len(fine_hist_normalized_np))])\n",
    "coarse_hist_normalized_np = np.concatenate([coarse_hist_normalized_np, np.zeros(longlength - len(coarse_hist_normalized_np))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8255d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.14344352],\n",
       "       [0.14344352, 1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(fine_hist_normalized_np, coarse_hist_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "263f9ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1384 artists>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(len(fine_hist_normalized_np)), fine_hist_normalized_np)\n",
    "plt.show()\n",
    "plt.bar(np.arange(len(coarse_hist_normalized_np)), coarse_hist_normalized_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234f3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "simCoef = np.corrcoef(fine_hist_normalized_np, coarse_hist_normalized_np)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eef0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4791f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_list_all = []\n",
    "hidden_channels = args.hidden_dim\n",
    "if dataset_name in ['brain']:\n",
    "    in_channels_nodes = 3\n",
    "    in_channels_edges = 1\n",
    "    num_classes = 8\n",
    "else:\n",
    "    try:\n",
    "        in_channels_nodes = dataset.num_node_features\n",
    "        in_channels_edges = dataset.num_edge_features\n",
    "        num_classes = dataset.num_classes\n",
    "    except:\n",
    "        in_channels_nodes = dataset_train.num_node_features\n",
    "        in_channels_edges = dataset_train.num_edge_features\n",
    "        num_classes = dataset_train.num_classes\n",
    "\n",
    "if dataset_name in ['MNIST', 'CIFAR10']:\n",
    "    in_channels_nodes += 2\n",
    "if in_channels_nodes == 0:\n",
    "    nodes_feature_flag = False\n",
    "    in_channels_nodes = 1\n",
    "else:\n",
    "    nodes_feature_flag = True\n",
    "\n",
    "if in_channels_edges == 0:\n",
    "    edge_feature_flag = False\n",
    "    in_channels_edges = 1\n",
    "else:\n",
    "    edge_feature_flag = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118a419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels_nodes, in_channels_edges, hidden_channels, num_classes, num_layers, edge_feature_flag):\n",
    "        super().__init__()\n",
    "        self.node_linear = Linear(in_channels_nodes, hidden_channels)\n",
    "        if edge_feature_flag:\n",
    "            self.edge_linear = Linear(in_channels_edges, hidden_channels)\n",
    "            #self.net1 = GINENet(in_channels_nodes, in_channels_edges, hidden_channels, hidden_channels, num_layers)\n",
    "            self.net1 = GINENet(hidden_channels, hidden_channels, hidden_channels, hidden_channels, num_layers)\n",
    "        else:\n",
    "            self.net1 = GINNet(hidden_channels, hidden_channels, hidden_channels, num_layers)\n",
    "            \n",
    "        self.net2 = GINENet(hidden_channels, hidden_channels, hidden_channels, hidden_channels, num_layers)\n",
    "        self.class_linear = Linear(hidden_channels, num_classes)\n",
    "        self.edge_feature_flag = edge_feature_flag\n",
    "        \n",
    "    def forward(self, \n",
    "                x_node, edge_index_node, edge_attr_node, batch_node,\n",
    "                x_edge, edge_index_edge, edge_attr_edge, batch_edge,\n",
    "                edge_index_cluster, batch_cluster):\n",
    "        x_node = self.node_linear(x_node)\n",
    "        x_edge = self.node_linear(x_edge)\n",
    "        if self.edge_feature_flag:\n",
    "            # with edge feature\n",
    "            edge_attr_node = self.edge_linear(edge_attr_node)\n",
    "            edge_attr_edge = self.edge_linear(edge_attr_edge)\n",
    "            edge_attr_cluster = self.net1(x_edge, edge_index_edge, edge_attr_edge, batch_edge)\n",
    "            x_cluster = self.net1(x_node, edge_index_node, edge_attr_node, batch_node)\n",
    "        else: \n",
    "            # no edge feature\n",
    "            edge_attr_cluster = self.net1(x_edge, edge_index_edge, batch_edge)\n",
    "            x_cluster = self.net1(x_node, edge_index_node, batch_node)\n",
    "            \n",
    "        out = self.net2(x_cluster, edge_index_cluster, edge_attr_cluster, batch_cluster)\n",
    "        out = self.class_linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8554de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClusterModel(\n",
    "    in_channels_nodes=in_channels_nodes, \n",
    "    in_channels_edges=in_channels_edges, \n",
    "    hidden_channels=hidden_channels, \n",
    "    num_classes=num_classes, \n",
    "    num_layers=args.number_layers,\n",
    "    edge_feature_flag=edge_feature_flag,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc0b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:'+str(args.gpu))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()#torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "def train(batched_dataset_node, batched_dataset_edge, batched_clustered_edge_index, batched_clustered_batch, batched_y_true):\n",
    "    loss_value_list = []\n",
    "    regularziation_value_list = []\n",
    "    model.train()\n",
    "    for data_node, data_edge, clustered_edge_index, clustered_batch, y_true in zip(batched_dataset_node, batched_dataset_edge, batched_clustered_edge_index, batched_clustered_batch, batched_y_true):\n",
    "        edge_index_node, batch_node = data_node.edge_index, data_node.batch\n",
    "        edge_index_edge, batch_edge = data_edge.edge_index, data_edge.batch\n",
    "        if nodes_feature_flag:\n",
    "            x_node = data_node.x.float()\n",
    "            x_edge = data_edge.x.float()\n",
    "        else:\n",
    "            x_node = torch.zeros((data_node.num_nodes, 1))\n",
    "            x_edge = torch.zeros((data_edge.num_nodes, 1))\n",
    "        if edge_feature_flag:\n",
    "            edge_attr_node = data_node.edge_attr.float()\n",
    "            edge_attr_edge = data_edge.edge_attr.float()\n",
    "        else:\n",
    "            edge_attr_node = torch.zeros((data_node.num_edges, 1))\n",
    "            edge_attr_edge = torch.zeros((data_edge.num_edges, 1))\n",
    "\n",
    "        x_node, edge_index_node, edge_attr_node, batch_node = x_node.to(device), edge_index_node.to(device), edge_attr_node.to(device), batch_node.to(device)\n",
    "        x_edge, edge_index_edge, edge_attr_edge, batch_edge = x_edge.to(device), edge_index_edge.to(device), edge_attr_edge.to(device), batch_edge.to(device) \n",
    "\n",
    "        if len(edge_attr_node.shape) == 1:\n",
    "            edge_attr_node = edge_attr_node.view(-1,1)\n",
    "            edge_attr_edge = edge_attr_edge.view(-1,1)\n",
    "        if dataset_name in ['MNIST', 'CIFAR10']:\n",
    "            pos_node = data_node.pos.float().to(device)\n",
    "            x_node = torch.cat([x_node, pos_node], dim=-1)\n",
    "            pos_edge = data_edge.pos.float().to(device)\n",
    "            x_edge = torch.cat([x_edge, pos_edge], dim=-1)\n",
    "        clustered_edge_index, clustered_batch = clustered_edge_index.to(device), clustered_batch.to(device)\n",
    "        y_true = y_true.view(-1).to(device)\n",
    "\n",
    "        out = model(\n",
    "            x_node, edge_index_node, edge_attr_node, batch_node,\n",
    "            x_edge, edge_index_edge, edge_attr_edge, batch_edge,\n",
    "            clustered_edge_index, clustered_batch\n",
    "        )\n",
    "        loss = criterion(out, y_true)  # Compute the loss.\n",
    "        loss_value = float(loss)\n",
    "        loss_value_list.append(loss_value)\n",
    "        \n",
    "        # compute the regularization term\n",
    "        regularization = 0\n",
    "        for para1, para2 in zip(model.net1.parameters(), model.net2.parameters()):\n",
    "            item = torch.sum((para1-para2)**2) \n",
    "            regularization += item\n",
    "        total_para = 0\n",
    "        for para in model.net1.parameters():\n",
    "            total_para += len(para.view(-1))\n",
    "        regularization = 10*simCoef * regularization / total_para\n",
    "        regularziation_value_list.append(float(regularization))\n",
    "        \n",
    "        loss = loss + regularization\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return loss_value_list, regularziation_value_list\n",
    "\n",
    "def test(batched_dataset_node, batched_dataset_edge, batched_clustered_edge_index, batched_clustered_batch, batched_y_true):\n",
    "    y_pred_list = []\n",
    "    model.eval()\n",
    "    for data_node, data_edge, clustered_edge_index, clustered_batch, y_true in zip(batched_dataset_node, batched_dataset_edge, batched_clustered_edge_index, batched_clustered_batch, batched_y_true):\n",
    "        edge_index_node, batch_node = data_node.edge_index, data_node.batch\n",
    "        edge_index_edge, batch_edge = data_edge.edge_index, data_edge.batch\n",
    "        if nodes_feature_flag:\n",
    "            x_node = data_node.x.float()\n",
    "            x_edge = data_edge.x.float()\n",
    "        else:\n",
    "            x_node = torch.zeros((data_node.num_nodes, 1))\n",
    "            x_edge = torch.zeros((data_edge.num_nodes, 1))\n",
    "        if edge_feature_flag:\n",
    "            edge_attr_node = data_node.edge_attr.float()\n",
    "            edge_attr_edge = data_edge.edge_attr.float()\n",
    "        else:\n",
    "            edge_attr_node = torch.zeros((data_node.num_edges, 1))\n",
    "            edge_attr_edge = torch.zeros((data_edge.num_edges, 1))\n",
    "\n",
    "        x_node, edge_index_node, edge_attr_node, batch_node = x_node.to(device), edge_index_node.to(device), edge_attr_node.to(device), batch_node.to(device)\n",
    "        x_edge, edge_index_edge, edge_attr_edge, batch_edge = x_edge.to(device), edge_index_edge.to(device), edge_attr_edge.to(device), batch_edge.to(device) \n",
    "\n",
    "        if len(edge_attr_node.shape) == 1:\n",
    "            edge_attr_node = edge_attr_node.view(-1,1)\n",
    "            edge_attr_edge = edge_attr_edge.view(-1,1)\n",
    "        if dataset_name in ['MNIST', 'CIFAR10']:\n",
    "            pos_node = data_node.pos.float().to(device)\n",
    "            x_node = torch.cat([x_node, pos_node], dim=-1)\n",
    "            pos_edge = data_edge.pos.float().to(device)\n",
    "            x_edge = torch.cat([x_edge, pos_edge], dim=-1)\n",
    "        clustered_edge_index, clustered_batch = clustered_edge_index.to(device), clustered_batch.to(device)\n",
    "        out = model(\n",
    "            x_node, edge_index_node, edge_attr_node, batch_node,\n",
    "            x_edge, edge_index_edge, edge_attr_edge, batch_edge,\n",
    "            clustered_edge_index, clustered_batch\n",
    "        )\n",
    "        #pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        #y_pred_list.append(pred.detach().cpu())\n",
    "        out = out.softmax(dim=1)\n",
    "        y_pred_list.append(out.detach().cpu())\n",
    "\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    return y_pred \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd155e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTEINS 5 1 64 2 0.0005\n",
      "Epoch, Loss, Reg Loss, Train Accuracy, Test Accuracy, Time\n",
      "000, 0.6554, 0.2505, 0.5958, 0.5946, 0.0856\n",
      "001, 0.6624, 0.2501, 0.6367, 0.6306, 0.1634\n",
      "002, 0.6085, 0.2497, 0.7146, 0.6667, 0.2388\n",
      "003, 0.6104, 0.2493, 0.7226, 0.6667, 0.3122\n",
      "004, 0.5904, 0.2489, 0.7335, 0.6667, 0.3834\n",
      "005, 0.5754, 0.2485, 0.7236, 0.6486, 0.4545\n",
      "006, 0.5735, 0.2481, 0.7345, 0.6577, 0.5220\n",
      "007, 0.5705, 0.2477, 0.7405, 0.6847, 0.5898\n",
      "008, 0.5602, 0.2473, 0.7335, 0.7027, 0.6581\n",
      "009, 0.5525, 0.2469, 0.7305, 0.6667, 0.7257\n",
      "010, 0.5502, 0.2465, 0.7365, 0.6847, 0.7928\n",
      "011, 0.5427, 0.2461, 0.7405, 0.6757, 0.8611\n",
      "012, 0.5341, 0.2457, 0.7415, 0.6847, 0.9286\n",
      "013, 0.5311, 0.2453, 0.7495, 0.6757, 0.9965\n",
      "014, 0.5257, 0.2449, 0.7495, 0.6937, 1.0645\n",
      "015, 0.5203, 0.2446, 0.7445, 0.6757, 1.1321\n",
      "016, 0.5191, 0.2442, 0.7505, 0.6847, 1.1997\n",
      "017, 0.5133, 0.2438, 0.7495, 0.7027, 1.2679\n",
      "018, 0.5091, 0.2434, 0.7565, 0.7027, 1.3353\n",
      "019, 0.5059, 0.2430, 0.7535, 0.7027, 1.4029\n",
      "020, 0.5015, 0.2426, 0.7475, 0.7027, 1.4711\n",
      "021, 0.5010, 0.2422, 0.7555, 0.7297, 1.5386\n",
      "022, 0.4970, 0.2419, 0.7565, 0.7117, 1.6060\n",
      "023, 0.4963, 0.2415, 0.7595, 0.7207, 1.6743\n",
      "024, 0.4934, 0.2411, 0.7575, 0.7207, 1.7420\n",
      "025, 0.4919, 0.2408, 0.7555, 0.7027, 1.8095\n",
      "026, 0.4907, 0.2404, 0.7575, 0.7027, 1.8777\n",
      "027, 0.4877, 0.2400, 0.7645, 0.7027, 1.9452\n",
      "028, 0.4882, 0.2397, 0.7595, 0.7027, 2.0128\n",
      "029, 0.4856, 0.2393, 0.7615, 0.6937, 2.0812\n",
      "030, 0.4850, 0.2390, 0.7605, 0.7117, 2.1482\n",
      "031, 0.4839, 0.2386, 0.7615, 0.7117, 2.2157\n",
      "032, 0.4822, 0.2383, 0.7655, 0.7027, 2.2839\n",
      "033, 0.4817, 0.2379, 0.7645, 0.7117, 2.3514\n",
      "034, 0.4800, 0.2376, 0.7635, 0.7027, 2.4187\n",
      "035, 0.4794, 0.2372, 0.7695, 0.6937, 2.4870\n",
      "036, 0.4792, 0.2369, 0.7685, 0.7027, 2.5545\n",
      "037, 0.4777, 0.2365, 0.7705, 0.7027, 2.6221\n",
      "038, 0.4774, 0.2362, 0.7705, 0.7117, 2.6901\n",
      "039, 0.4777, 0.2359, 0.7695, 0.7027, 2.7577\n",
      "040, 0.4764, 0.2355, 0.7695, 0.7027, 2.8252\n",
      "041, 0.4751, 0.2352, 0.7705, 0.7027, 2.8933\n",
      "042, 0.4741, 0.2349, 0.7685, 0.6937, 2.9604\n",
      "043, 0.4740, 0.2345, 0.7725, 0.7027, 3.0280\n",
      "044, 0.4741, 0.2342, 0.7665, 0.6937, 3.0962\n",
      "045, 0.4735, 0.2339, 0.7705, 0.7117, 3.1636\n",
      "046, 0.4730, 0.2335, 0.7665, 0.6937, 3.2311\n",
      "047, 0.4715, 0.2332, 0.7685, 0.7027, 3.2994\n",
      "048, 0.4706, 0.2329, 0.7685, 0.7207, 3.3669\n",
      "049, 0.4703, 0.2326, 0.7645, 0.7117, 3.4344\n",
      "050, 0.4702, 0.2322, 0.7665, 0.7207, 3.5030\n",
      "051, 0.4709, 0.2319, 0.7665, 0.7117, 3.5705\n",
      "052, 0.4710, 0.2316, 0.7695, 0.7207, 3.6381\n",
      "053, 0.4724, 0.2313, 0.7695, 0.7207, 3.7065\n",
      "054, 0.4688, 0.2310, 0.7675, 0.7117, 3.7740\n",
      "055, 0.4670, 0.2306, 0.7695, 0.7117, 3.8415\n",
      "056, 0.4678, 0.2303, 0.7705, 0.7207, 3.9096\n",
      "057, 0.4677, 0.2300, 0.7725, 0.7117, 3.9773\n",
      "058, 0.4666, 0.2297, 0.7685, 0.7117, 4.0445\n",
      "059, 0.4648, 0.2294, 0.7695, 0.7207, 4.1127\n",
      "060, 0.4647, 0.2291, 0.7715, 0.7207, 4.1803\n",
      "061, 0.4660, 0.2288, 0.7715, 0.7207, 4.2478\n",
      "062, 0.4653, 0.2284, 0.7754, 0.7207, 4.3161\n",
      "063, 0.4657, 0.2281, 0.7725, 0.7207, 4.3836\n",
      "064, 0.4627, 0.2278, 0.7715, 0.7117, 4.4511\n",
      "065, 0.4616, 0.2275, 0.7745, 0.7207, 4.5192\n",
      "066, 0.4624, 0.2272, 0.7725, 0.7207, 4.5868\n",
      "067, 0.4624, 0.2269, 0.7745, 0.7207, 4.6543\n",
      "068, 0.4625, 0.2266, 0.7715, 0.7207, 4.7225\n",
      "069, 0.4603, 0.2263, 0.7754, 0.7207, 4.7901\n",
      "070, 0.4590, 0.2260, 0.7725, 0.7207, 4.8578\n",
      "071, 0.4588, 0.2257, 0.7745, 0.7207, 4.9261\n",
      "072, 0.4595, 0.2254, 0.7735, 0.7207, 4.9936\n",
      "073, 0.4626, 0.2251, 0.7735, 0.7207, 5.0612\n",
      "074, 0.4610, 0.2248, 0.7735, 0.7207, 5.1294\n",
      "075, 0.4607, 0.2245, 0.7754, 0.7207, 5.1969\n",
      "076, 0.4562, 0.2242, 0.7754, 0.7207, 5.2645\n",
      "077, 0.4575, 0.2239, 0.7735, 0.7117, 5.3328\n",
      "078, 0.4635, 0.2236, 0.7774, 0.7207, 5.4001\n",
      "079, 0.4576, 0.2233, 0.7745, 0.7297, 5.4675\n",
      "080, 0.4550, 0.2230, 0.7794, 0.7117, 5.5358\n",
      "081, 0.4539, 0.2228, 0.7745, 0.7207, 5.6035\n",
      "082, 0.4550, 0.2225, 0.7735, 0.7297, 5.6708\n",
      "083, 0.4558, 0.2222, 0.7804, 0.7207, 5.7390\n",
      "084, 0.4529, 0.2219, 0.7814, 0.7117, 5.8065\n",
      "085, 0.4513, 0.2216, 0.7804, 0.7117, 5.8742\n",
      "086, 0.4511, 0.2213, 0.7784, 0.7207, 5.9426\n",
      "087, 0.4515, 0.2210, 0.7794, 0.7207, 6.0101\n",
      "088, 0.4517, 0.2207, 0.7814, 0.7207, 6.0776\n",
      "089, 0.4504, 0.2205, 0.7814, 0.7207, 6.1461\n",
      "090, 0.4497, 0.2202, 0.7824, 0.7207, 6.2134\n",
      "091, 0.4486, 0.2199, 0.7784, 0.7207, 6.2810\n",
      "092, 0.4480, 0.2196, 0.7804, 0.7207, 6.3491\n",
      "093, 0.4475, 0.2193, 0.7804, 0.7297, 6.4166\n",
      "094, 0.4477, 0.2190, 0.7834, 0.7117, 6.4841\n",
      "095, 0.4487, 0.2188, 0.7794, 0.7207, 6.5525\n",
      "096, 0.4590, 0.2185, 0.7764, 0.6937, 6.6201\n",
      "097, 0.4601, 0.2182, 0.7754, 0.7297, 6.6879\n",
      "098, 0.4710, 0.2179, 0.7824, 0.7117, 6.7559\n",
      "099, 0.4470, 0.2176, 0.7565, 0.7207, 6.8234\n",
      "100, 0.4759, 0.2174, 0.7615, 0.7117, 6.8910\n",
      "101, 0.5017, 0.2171, 0.7725, 0.7117, 6.9592\n",
      "102, 0.4811, 0.2168, 0.7545, 0.6757, 7.0263\n",
      "103, 0.4976, 0.2166, 0.7725, 0.7297, 7.0938\n",
      "104, 0.4601, 0.2163, 0.7754, 0.7207, 7.1620\n",
      "105, 0.4718, 0.2161, 0.7754, 0.7117, 7.2296\n",
      "106, 0.4632, 0.2158, 0.7735, 0.7207, 7.2971\n",
      "107, 0.4675, 0.2155, 0.7794, 0.7117, 7.3654\n",
      "108, 0.4584, 0.2153, 0.7774, 0.7207, 7.4330\n",
      "109, 0.4563, 0.2150, 0.7735, 0.7207, 7.5005\n",
      "110, 0.4625, 0.2148, 0.7735, 0.7117, 7.5687\n",
      "111, 0.4587, 0.2145, 0.7735, 0.7117, 7.6362\n",
      "112, 0.4567, 0.2142, 0.7784, 0.7207, 7.7039\n",
      "113, 0.4536, 0.2140, 0.7814, 0.7117, 7.7720\n",
      "114, 0.4518, 0.2137, 0.7784, 0.7297, 7.8393\n",
      "115, 0.4526, 0.2135, 0.7814, 0.7117, 7.9068\n",
      "116, 0.4513, 0.2132, 0.7814, 0.7207, 7.9751\n",
      "117, 0.4517, 0.2129, 0.7844, 0.7207, 8.0425\n",
      "118, 0.4485, 0.2127, 0.7804, 0.7207, 8.1100\n",
      "119, 0.4488, 0.2124, 0.7774, 0.7297, 8.1782\n",
      "120, 0.4492, 0.2121, 0.7745, 0.7207, 8.2457\n",
      "121, 0.4472, 0.2119, 0.7814, 0.7117, 8.3134\n",
      "122, 0.4443, 0.2116, 0.7804, 0.7207, 8.3816\n",
      "123, 0.4442, 0.2114, 0.7834, 0.7207, 8.4491\n",
      "124, 0.4448, 0.2111, 0.7834, 0.7117, 8.5165\n",
      "125, 0.4439, 0.2109, 0.7834, 0.7117, 8.5850\n",
      "126, 0.4428, 0.2106, 0.7804, 0.7117, 8.6525\n",
      "127, 0.4421, 0.2103, 0.7854, 0.7117, 8.7200\n",
      "128, 0.4415, 0.2101, 0.7844, 0.7117, 8.7883\n",
      "129, 0.4405, 0.2098, 0.7844, 0.7117, 8.8558\n",
      "130, 0.4393, 0.2095, 0.7884, 0.7117, 8.9233\n",
      "131, 0.4390, 0.2093, 0.7884, 0.7207, 8.9916\n",
      "132, 0.4390, 0.2090, 0.7894, 0.7117, 9.0591\n",
      "133, 0.4377, 0.2088, 0.7884, 0.7027, 9.1266\n",
      "134, 0.4367, 0.2085, 0.7874, 0.7027, 9.1947\n",
      "135, 0.4362, 0.2083, 0.7854, 0.7117, 9.2622\n",
      "136, 0.4352, 0.2080, 0.7884, 0.7117, 9.3297\n",
      "137, 0.4344, 0.2077, 0.7914, 0.7027, 9.3982\n",
      "138, 0.4340, 0.2075, 0.7934, 0.7027, 9.4656\n",
      "139, 0.4332, 0.2072, 0.7894, 0.7117, 9.5332\n",
      "140, 0.4322, 0.2070, 0.7884, 0.7117, 9.6013\n",
      "141, 0.4315, 0.2067, 0.7894, 0.7117, 9.6690\n",
      "142, 0.4309, 0.2065, 0.7904, 0.7027, 9.7363\n",
      "143, 0.4302, 0.2062, 0.7944, 0.7027, 9.8046\n",
      "144, 0.4296, 0.2060, 0.7934, 0.7027, 9.8722\n",
      "145, 0.4289, 0.2057, 0.7924, 0.7027, 9.9397\n",
      "146, 0.4281, 0.2054, 0.7924, 0.7027, 10.0078\n",
      "147, 0.4275, 0.2052, 0.7964, 0.7027, 10.0753\n",
      "148, 0.4266, 0.2049, 0.7944, 0.7027, 10.1428\n",
      "149, 0.4259, 0.2047, 0.7974, 0.7027, 10.2113\n",
      "150, 0.4251, 0.2044, 0.7934, 0.7027, 10.2788\n",
      "151, 0.4243, 0.2042, 0.7974, 0.7027, 10.3466\n",
      "152, 0.4240, 0.2039, 0.7934, 0.7387, 10.4148\n",
      "153, 0.4249, 0.2037, 0.7924, 0.7027, 10.4823\n",
      "154, 0.4308, 0.2034, 0.7705, 0.7117, 10.5499\n",
      "155, 0.4770, 0.2032, 0.7774, 0.7207, 10.6185\n",
      "156, 0.4479, 0.2030, 0.7904, 0.7027, 10.6861\n",
      "157, 0.4272, 0.2027, 0.7764, 0.7297, 10.7535\n",
      "158, 0.4409, 0.2025, 0.7914, 0.6937, 10.8220\n",
      "159, 0.4348, 0.2022, 0.7944, 0.7027, 10.8895\n",
      "160, 0.4258, 0.2020, 0.7944, 0.7027, 10.9570\n",
      "161, 0.4243, 0.2017, 0.7924, 0.7117, 11.0252\n",
      "162, 0.4270, 0.2015, 0.7914, 0.7117, 11.0930\n",
      "163, 0.4223, 0.2013, 0.7914, 0.7117, 11.1604\n",
      "164, 0.4214, 0.2010, 0.7944, 0.6937, 11.2288\n",
      "165, 0.4237, 0.2008, 0.7864, 0.7297, 11.2965\n",
      "166, 0.4236, 0.2005, 0.7994, 0.6937, 11.3644\n",
      "167, 0.4219, 0.2003, 0.7934, 0.7027, 11.4329\n",
      "168, 0.4225, 0.2001, 0.7944, 0.7027, 11.5007\n",
      "169, 0.4174, 0.1998, 0.8004, 0.7027, 11.5684\n",
      "170, 0.4175, 0.1996, 0.7844, 0.7207, 11.6369\n",
      "171, 0.4203, 0.1993, 0.8034, 0.7117, 11.7047\n",
      "172, 0.4171, 0.1991, 0.7974, 0.7027, 11.7727\n",
      "173, 0.4142, 0.1989, 0.7944, 0.7027, 11.8405\n",
      "174, 0.4140, 0.1986, 0.7984, 0.7027, 11.9083\n",
      "175, 0.4122, 0.1984, 0.7914, 0.7117, 11.9761\n",
      "176, 0.4127, 0.1982, 0.8014, 0.6937, 12.0449\n",
      "177, 0.4137, 0.1979, 0.7884, 0.7207, 12.1126\n",
      "178, 0.4143, 0.1977, 0.8024, 0.7117, 12.1803\n",
      "179, 0.4163, 0.1974, 0.7914, 0.7207, 12.2490\n",
      "180, 0.4194, 0.1972, 0.8064, 0.7027, 12.3166\n",
      "181, 0.4124, 0.1970, 0.7914, 0.7117, 12.3846\n",
      "182, 0.4098, 0.1967, 0.8044, 0.6937, 12.4532\n",
      "183, 0.4066, 0.1965, 0.8064, 0.6937, 12.5209\n",
      "184, 0.4051, 0.1963, 0.7954, 0.7027, 12.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185, 0.4065, 0.1960, 0.8074, 0.7027, 12.6574\n",
      "186, 0.4083, 0.1958, 0.7864, 0.7117, 12.7251\n",
      "187, 0.4226, 0.1956, 0.7854, 0.7207, 12.7929\n",
      "188, 0.4445, 0.1953, 0.7675, 0.6847, 12.8616\n",
      "189, 0.5774, 0.1951, 0.7834, 0.7387, 12.9295\n",
      "190, 0.4576, 0.1949, 0.7246, 0.6847, 12.9975\n",
      "191, 0.5672, 0.1946, 0.7994, 0.7207, 13.0654\n",
      "192, 0.4359, 0.1944, 0.7615, 0.6847, 13.1332\n",
      "193, 0.4858, 0.1942, 0.7655, 0.6757, 13.2010\n",
      "194, 0.4540, 0.1940, 0.7595, 0.6577, 13.2699\n",
      "195, 0.4874, 0.1938, 0.7665, 0.7207, 13.3376\n",
      "196, 0.4805, 0.1936, 0.7695, 0.7117, 13.4054\n",
      "197, 0.4947, 0.1933, 0.7705, 0.7027, 13.4740\n",
      "198, 0.4862, 0.1931, 0.7655, 0.7207, 13.5416\n",
      "199, 0.4933, 0.1929, 0.7645, 0.6847, 13.6095\n",
      "200, 0.4668, 0.1927, 0.7774, 0.6847, 13.6780\n",
      "201, 0.4583, 0.1925, 0.7804, 0.6937, 13.7458\n",
      "202, 0.4677, 0.1922, 0.7864, 0.6937, 13.8134\n",
      "203, 0.4612, 0.1920, 0.7745, 0.6847, 13.8820\n",
      "204, 0.4567, 0.1918, 0.7695, 0.6847, 13.9497\n",
      "205, 0.4615, 0.1916, 0.7735, 0.6937, 14.0175\n",
      "206, 0.4479, 0.1913, 0.7804, 0.7297, 14.0859\n",
      "207, 0.4475, 0.1911, 0.7764, 0.7117, 14.1531\n",
      "208, 0.4515, 0.1909, 0.7764, 0.7297, 14.2211\n",
      "209, 0.4453, 0.1907, 0.7814, 0.7027, 14.2896\n",
      "210, 0.4449, 0.1905, 0.7854, 0.6937, 14.3572\n",
      "211, 0.4443, 0.1902, 0.7864, 0.7117, 14.4247\n",
      "212, 0.4384, 0.1900, 0.7824, 0.7117, 14.4935\n",
      "213, 0.4389, 0.1898, 0.7854, 0.7117, 14.5612\n",
      "214, 0.4362, 0.1896, 0.7894, 0.7117, 14.6289\n",
      "215, 0.4317, 0.1893, 0.7944, 0.7027, 14.6975\n",
      "216, 0.4327, 0.1891, 0.7964, 0.6937, 14.7650\n",
      "217, 0.4316, 0.1889, 0.7874, 0.7207, 14.8326\n",
      "218, 0.4299, 0.1887, 0.7844, 0.7117, 14.9012\n",
      "219, 0.4303, 0.1885, 0.7854, 0.7207, 14.9702\n",
      "220, 0.4269, 0.1882, 0.7924, 0.7117, 15.0380\n",
      "221, 0.4250, 0.1880, 0.7934, 0.7387, 15.1065\n",
      "222, 0.4232, 0.1878, 0.7844, 0.7297, 15.1743\n",
      "223, 0.4216, 0.1876, 0.7854, 0.7297, 15.2420\n",
      "224, 0.4212, 0.1873, 0.7904, 0.7297, 15.3108\n",
      "225, 0.4193, 0.1871, 0.7894, 0.7027, 15.3784\n",
      "226, 0.4188, 0.1869, 0.7924, 0.7027, 15.4463\n",
      "227, 0.4176, 0.1867, 0.7894, 0.7027, 15.5148\n",
      "228, 0.4164, 0.1865, 0.7894, 0.7027, 15.5827\n",
      "229, 0.4151, 0.1862, 0.7944, 0.6937, 15.6505\n",
      "230, 0.4134, 0.1860, 0.7974, 0.7027, 15.7191\n",
      "231, 0.4126, 0.1858, 0.7954, 0.7027, 15.7868\n",
      "232, 0.4112, 0.1856, 0.7944, 0.7027, 15.8545\n",
      "233, 0.4107, 0.1854, 0.7944, 0.7027, 15.9232\n",
      "234, 0.4097, 0.1851, 0.8004, 0.6937, 15.9908\n",
      "235, 0.4088, 0.1849, 0.8014, 0.7027, 16.0588\n",
      "236, 0.4073, 0.1847, 0.8004, 0.7027, 16.1273\n",
      "237, 0.4063, 0.1845, 0.7994, 0.7027, 16.1951\n",
      "238, 0.4052, 0.1843, 0.8014, 0.6937, 16.2631\n",
      "239, 0.4044, 0.1840, 0.8024, 0.6937, 16.3309\n",
      "240, 0.4035, 0.1838, 0.8034, 0.7027, 16.3987\n",
      "241, 0.4024, 0.1836, 0.8014, 0.7027, 16.4664\n",
      "242, 0.4014, 0.1834, 0.8044, 0.7027, 16.5349\n",
      "243, 0.4005, 0.1832, 0.8044, 0.7027, 16.6022\n",
      "244, 0.3995, 0.1830, 0.8034, 0.7027, 16.6700\n",
      "245, 0.3986, 0.1827, 0.8074, 0.7027, 16.7385\n",
      "246, 0.3975, 0.1825, 0.8094, 0.7027, 16.8068\n",
      "247, 0.3965, 0.1823, 0.8064, 0.7027, 16.8745\n",
      "248, 0.3956, 0.1821, 0.8084, 0.7027, 16.9425\n",
      "249, 0.3948, 0.1819, 0.8104, 0.7027, 17.0102\n",
      "250, 0.3938, 0.1817, 0.8094, 0.7027, 17.0780\n",
      "251, 0.3928, 0.1815, 0.8104, 0.7027, 17.1468\n",
      "252, 0.3920, 0.1812, 0.8104, 0.7027, 17.2143\n",
      "253, 0.3911, 0.1810, 0.8124, 0.7027, 17.2820\n",
      "254, 0.3901, 0.1808, 0.8124, 0.7027, 17.3505\n",
      "255, 0.3893, 0.1806, 0.8114, 0.7117, 17.4183\n",
      "256, 0.3884, 0.1804, 0.8134, 0.7117, 17.4858\n",
      "257, 0.3875, 0.1802, 0.8144, 0.7117, 17.5546\n",
      "258, 0.3867, 0.1800, 0.8144, 0.7117, 17.6224\n",
      "259, 0.3858, 0.1797, 0.8134, 0.7207, 17.6900\n",
      "260, 0.3850, 0.1795, 0.8144, 0.7117, 17.7587\n",
      "261, 0.3841, 0.1793, 0.8144, 0.7117, 17.8264\n",
      "262, 0.3831, 0.1791, 0.8164, 0.7117, 17.8942\n",
      "263, 0.3822, 0.1789, 0.8164, 0.7117, 17.9626\n",
      "264, 0.3813, 0.1787, 0.8174, 0.7117, 18.0306\n",
      "265, 0.3804, 0.1785, 0.8154, 0.7027, 18.0981\n",
      "266, 0.3796, 0.1783, 0.8174, 0.7117, 18.1666\n",
      "267, 0.3788, 0.1781, 0.8204, 0.7027, 18.2344\n",
      "268, 0.3781, 0.1778, 0.8204, 0.7117, 18.3021\n",
      "269, 0.3778, 0.1776, 0.8224, 0.7027, 18.3701\n",
      "270, 0.3780, 0.1774, 0.8184, 0.7117, 18.4378\n",
      "271, 0.3795, 0.1772, 0.8244, 0.7027, 18.5055\n",
      "272, 0.3823, 0.1770, 0.8054, 0.7117, 18.5740\n",
      "273, 0.3917, 0.1768, 0.8104, 0.6937, 18.6419\n",
      "274, 0.3959, 0.1766, 0.8024, 0.7297, 18.7098\n",
      "275, 0.4067, 0.1764, 0.8134, 0.7027, 18.7777\n",
      "276, 0.3796, 0.1762, 0.8214, 0.7027, 18.8454\n",
      "277, 0.3828, 0.1760, 0.8104, 0.7117, 18.9134\n",
      "278, 0.4009, 0.1758, 0.8244, 0.7027, 18.9813\n",
      "279, 0.3759, 0.1756, 0.8214, 0.7027, 19.0491\n",
      "280, 0.3773, 0.1754, 0.8014, 0.7117, 19.1168\n",
      "281, 0.3922, 0.1752, 0.8204, 0.7027, 19.1855\n",
      "282, 0.3788, 0.1750, 0.8283, 0.7027, 19.2531\n",
      "283, 0.3718, 0.1748, 0.8164, 0.7297, 19.3210\n",
      "284, 0.3826, 0.1746, 0.8263, 0.6937, 19.3895\n",
      "285, 0.3770, 0.1744, 0.8174, 0.7027, 19.4570\n",
      "286, 0.3720, 0.1742, 0.8204, 0.7117, 19.5248\n",
      "287, 0.3693, 0.1740, 0.8293, 0.7027, 19.5936\n",
      "288, 0.3694, 0.1737, 0.8184, 0.7027, 19.6613\n",
      "289, 0.3718, 0.1735, 0.8253, 0.7117, 19.7290\n",
      "290, 0.3679, 0.1733, 0.8303, 0.7117, 19.7975\n",
      "291, 0.3643, 0.1731, 0.8283, 0.7207, 19.8651\n",
      "292, 0.3625, 0.1729, 0.8253, 0.7027, 19.9329\n",
      "293, 0.3643, 0.1727, 0.8234, 0.7207, 20.0013\n",
      "294, 0.3632, 0.1725, 0.8333, 0.6937, 20.0696\n",
      "295, 0.3605, 0.1723, 0.8323, 0.7207, 20.1373\n",
      "296, 0.3593, 0.1721, 0.8283, 0.7027, 20.2053\n",
      "297, 0.3589, 0.1719, 0.8303, 0.6937, 20.2731\n",
      "298, 0.3589, 0.1717, 0.8313, 0.7207, 20.3407\n",
      "299, 0.3589, 0.1715, 0.8363, 0.7027, 20.4092\n",
      "300, 0.3585, 0.1713, 0.8273, 0.7297, 20.4769\n",
      "301, 0.3574, 0.1711, 0.8333, 0.6847, 20.5447\n",
      "302, 0.3583, 0.1709, 0.8214, 0.7207, 20.6132\n",
      "303, 0.3616, 0.1707, 0.8234, 0.6937, 20.6813\n",
      "304, 0.3638, 0.1705, 0.8204, 0.7297, 20.7487\n",
      "305, 0.3768, 0.1703, 0.8154, 0.6757, 20.8169\n",
      "306, 0.3785, 0.1701, 0.7984, 0.7117, 20.8844\n",
      "307, 0.4095, 0.1699, 0.8174, 0.6847, 20.9522\n",
      "308, 0.3752, 0.1697, 0.8313, 0.6847, 21.0202\n",
      "309, 0.3544, 0.1696, 0.8194, 0.7297, 21.0880\n",
      "310, 0.3608, 0.1694, 0.8343, 0.6937, 21.1556\n",
      "311, 0.3688, 0.1692, 0.8094, 0.7297, 21.2234\n",
      "312, 0.3871, 0.1690, 0.8204, 0.7027, 21.2908\n",
      "313, 0.3721, 0.1688, 0.8353, 0.6937, 21.3586\n",
      "314, 0.3510, 0.1686, 0.8234, 0.7207, 21.4266\n",
      "315, 0.3658, 0.1684, 0.8224, 0.6937, 21.4941\n",
      "316, 0.3755, 0.1682, 0.8084, 0.7297, 21.5616\n",
      "317, 0.4005, 0.1680, 0.8044, 0.6937, 21.6298\n",
      "318, 0.3827, 0.1678, 0.8214, 0.7027, 21.6974\n",
      "319, 0.3669, 0.1676, 0.8154, 0.7117, 21.7649\n",
      "320, 0.3791, 0.1674, 0.8234, 0.6667, 21.8328\n",
      "321, 0.3605, 0.1672, 0.8253, 0.6937, 21.9007\n",
      "322, 0.3647, 0.1670, 0.8144, 0.7207, 21.9682\n",
      "323, 0.3711, 0.1668, 0.8283, 0.6667, 22.0364\n",
      "324, 0.3549, 0.1667, 0.8244, 0.6937, 22.1040\n",
      "325, 0.3640, 0.1665, 0.8094, 0.7207, 22.1717\n",
      "326, 0.3746, 0.1663, 0.8443, 0.7027, 22.2397\n",
      "327, 0.3512, 0.1661, 0.8363, 0.6937, 22.3072\n",
      "328, 0.3510, 0.1659, 0.8343, 0.7117, 22.3747\n",
      "329, 0.3540, 0.1657, 0.8403, 0.7207, 22.4428\n",
      "330, 0.3488, 0.1655, 0.8413, 0.7117, 22.5104\n",
      "331, 0.3463, 0.1653, 0.8443, 0.7117, 22.5780\n",
      "332, 0.3489, 0.1651, 0.8403, 0.7027, 22.6462\n",
      "333, 0.3432, 0.1649, 0.8433, 0.7207, 22.7140\n",
      "334, 0.3407, 0.1647, 0.8323, 0.7207, 22.7816\n",
      "335, 0.3440, 0.1645, 0.8463, 0.6937, 22.8496\n",
      "336, 0.3365, 0.1644, 0.8483, 0.6847, 22.9173\n",
      "337, 0.3375, 0.1642, 0.8453, 0.7027, 22.9851\n",
      "338, 0.3375, 0.1640, 0.8483, 0.7027, 23.0530\n",
      "339, 0.3368, 0.1638, 0.8433, 0.6937, 23.1206\n",
      "340, 0.3323, 0.1636, 0.8493, 0.6847, 23.1883\n",
      "341, 0.3336, 0.1634, 0.8533, 0.7027, 23.2562\n",
      "342, 0.3313, 0.1632, 0.8393, 0.7117, 23.3238\n",
      "343, 0.3326, 0.1630, 0.8433, 0.6937, 23.3913\n",
      "344, 0.3356, 0.1628, 0.8353, 0.7387, 23.4595\n",
      "345, 0.3374, 0.1626, 0.8373, 0.6937, 23.5272\n",
      "346, 0.3399, 0.1625, 0.8343, 0.7387, 23.5948\n",
      "347, 0.3444, 0.1623, 0.8303, 0.6486, 23.6628\n",
      "348, 0.3516, 0.1621, 0.8044, 0.7117, 23.7305\n",
      "349, 0.4003, 0.1619, 0.8204, 0.6757, 23.7980\n",
      "350, 0.3642, 0.1617, 0.8353, 0.6937, 23.8659\n",
      "351, 0.3445, 0.1615, 0.8483, 0.7117, 23.9335\n",
      "352, 0.3377, 0.1613, 0.8383, 0.6847, 24.0013\n",
      "353, 0.3604, 0.1612, 0.8114, 0.7207, 24.0691\n",
      "354, 0.3828, 0.1610, 0.8214, 0.6847, 24.1367\n",
      "355, 0.3743, 0.1608, 0.8373, 0.6757, 24.2042\n",
      "356, 0.3480, 0.1606, 0.8323, 0.7207, 24.2723\n",
      "357, 0.3495, 0.1604, 0.8323, 0.6577, 24.3398\n",
      "358, 0.3679, 0.1602, 0.8084, 0.6937, 24.4074\n",
      "359, 0.3969, 0.1601, 0.8333, 0.6757, 24.4753\n",
      "360, 0.3450, 0.1599, 0.8174, 0.6937, 24.5431\n",
      "361, 0.3589, 0.1597, 0.8114, 0.7027, 24.6107\n",
      "362, 0.3738, 0.1595, 0.8513, 0.7027, 24.6786\n",
      "363, 0.3312, 0.1593, 0.8194, 0.6667, 24.7463\n",
      "364, 0.3705, 0.1591, 0.8074, 0.7117, 24.8140\n",
      "365, 0.4280, 0.1590, 0.8253, 0.7027, 24.8822\n",
      "366, 0.3560, 0.1588, 0.8054, 0.6667, 24.9498\n",
      "367, 0.4011, 0.1586, 0.8303, 0.6667, 25.0174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368, 0.3534, 0.1584, 0.8244, 0.6937, 25.0855\n",
      "369, 0.3740, 0.1583, 0.8473, 0.7027, 25.1530\n",
      "370, 0.3465, 0.1581, 0.8323, 0.6937, 25.2206\n",
      "371, 0.3619, 0.1579, 0.8134, 0.7207, 25.2885\n",
      "372, 0.3889, 0.1577, 0.8253, 0.6847, 25.3564\n",
      "373, 0.3652, 0.1575, 0.8114, 0.6847, 25.4238\n",
      "374, 0.3808, 0.1573, 0.8403, 0.6847, 25.4917\n",
      "375, 0.3365, 0.1572, 0.8244, 0.7207, 25.5593\n",
      "376, 0.3675, 0.1570, 0.8523, 0.7207, 25.6270\n",
      "377, 0.3415, 0.1568, 0.8563, 0.6847, 25.6950\n",
      "378, 0.3539, 0.1566, 0.8313, 0.7207, 25.7625\n",
      "379, 0.3410, 0.1564, 0.8283, 0.6937, 25.8301\n",
      "380, 0.3553, 0.1563, 0.8343, 0.7027, 25.8982\n",
      "381, 0.3538, 0.1561, 0.8353, 0.6937, 25.9658\n",
      "382, 0.3366, 0.1559, 0.8453, 0.7027, 26.0333\n",
      "383, 0.3321, 0.1557, 0.8453, 0.6847, 26.1013\n",
      "384, 0.3400, 0.1555, 0.8443, 0.6937, 26.1692\n",
      "385, 0.3432, 0.1554, 0.8553, 0.7117, 26.2370\n",
      "386, 0.3249, 0.1552, 0.8513, 0.7027, 26.3050\n",
      "387, 0.3234, 0.1550, 0.8413, 0.6937, 26.3726\n",
      "388, 0.3344, 0.1548, 0.8523, 0.6757, 26.4404\n",
      "389, 0.3271, 0.1546, 0.8533, 0.7297, 26.5083\n",
      "390, 0.3195, 0.1545, 0.8713, 0.7027, 26.5759\n",
      "391, 0.3179, 0.1543, 0.8623, 0.7027, 26.6435\n",
      "392, 0.3198, 0.1541, 0.8473, 0.7297, 26.7116\n",
      "393, 0.3219, 0.1539, 0.8663, 0.6847, 26.7793\n",
      "394, 0.3112, 0.1537, 0.8583, 0.6937, 26.8470\n",
      "395, 0.3149, 0.1536, 0.8563, 0.7027, 26.9155\n",
      "396, 0.3133, 0.1534, 0.8573, 0.6937, 26.9833\n",
      "397, 0.3113, 0.1532, 0.8633, 0.6937, 27.0507\n",
      "398, 0.3090, 0.1530, 0.8663, 0.7027, 27.1192\n",
      "399, 0.3047, 0.1529, 0.8523, 0.7207, 27.1872\n",
      "400, 0.3090, 0.1527, 0.8643, 0.6937, 27.2550\n",
      "401, 0.3042, 0.1525, 0.8683, 0.6937, 27.3235\n",
      "402, 0.3006, 0.1523, 0.8603, 0.7027, 27.3914\n",
      "403, 0.3025, 0.1522, 0.8683, 0.6937, 27.4590\n",
      "404, 0.2993, 0.1520, 0.8673, 0.6937, 27.5308\n",
      "405, 0.2979, 0.1518, 0.8653, 0.6937, 27.5987\n",
      "406, 0.2977, 0.1516, 0.8713, 0.7027, 27.6665\n",
      "407, 0.2956, 0.1515, 0.8752, 0.7027, 27.7353\n",
      "408, 0.2944, 0.1513, 0.8663, 0.6937, 27.8031\n",
      "409, 0.2939, 0.1511, 0.8683, 0.6847, 27.8709\n",
      "410, 0.2925, 0.1509, 0.8693, 0.6937, 27.9394\n",
      "411, 0.2902, 0.1508, 0.8723, 0.6937, 28.0079\n",
      "412, 0.2894, 0.1506, 0.8792, 0.7027, 28.0757\n",
      "413, 0.2890, 0.1504, 0.8713, 0.6937, 28.1438\n",
      "414, 0.2874, 0.1502, 0.8762, 0.7027, 28.2118\n",
      "415, 0.2854, 0.1501, 0.8723, 0.6937, 28.2796\n",
      "416, 0.2842, 0.1499, 0.8693, 0.6937, 28.3481\n",
      "417, 0.2835, 0.1497, 0.8762, 0.7027, 28.4158\n",
      "418, 0.2825, 0.1495, 0.8752, 0.6937, 28.4837\n",
      "419, 0.2819, 0.1494, 0.8782, 0.6937, 28.5521\n",
      "420, 0.2812, 0.1492, 0.8792, 0.6937, 28.6208\n",
      "421, 0.2799, 0.1490, 0.8782, 0.6937, 28.6887\n",
      "422, 0.2787, 0.1488, 0.8762, 0.6937, 28.7566\n",
      "423, 0.2771, 0.1487, 0.8802, 0.7027, 28.8245\n",
      "424, 0.2756, 0.1485, 0.8822, 0.7027, 28.8923\n",
      "425, 0.2750, 0.1483, 0.8802, 0.6937, 28.9610\n",
      "426, 0.2748, 0.1481, 0.8772, 0.7117, 29.0284\n",
      "427, 0.2773, 0.1480, 0.8663, 0.6847, 29.0963\n",
      "428, 0.2868, 0.1478, 0.8423, 0.6847, 29.1647\n",
      "429, 0.3340, 0.1476, 0.7725, 0.6216, 29.2327\n",
      "430, 0.4677, 0.1475, 0.7365, 0.6396, 29.3004\n",
      "431, 1.0694, 0.1473, 0.8054, 0.6847, 29.3692\n",
      "432, 0.5834, 0.1471, 0.6727, 0.6036, 29.4371\n",
      "433, 1.0636, 0.1470, 0.7974, 0.7027, 29.5050\n",
      "434, 0.5691, 0.1468, 0.7555, 0.6577, 29.5734\n",
      "435, 0.7385, 0.1467, 0.7515, 0.6486, 29.6409\n",
      "436, 0.6918, 0.1465, 0.7904, 0.6577, 29.7087\n",
      "437, 0.5318, 0.1464, 0.7764, 0.6847, 29.7772\n",
      "438, 0.5040, 0.1462, 0.7824, 0.6486, 29.8455\n",
      "439, 0.4574, 0.1461, 0.7824, 0.6306, 29.9133\n",
      "440, 0.4633, 0.1459, 0.7844, 0.6486, 29.9815\n",
      "441, 0.4710, 0.1458, 0.7824, 0.7117, 30.0494\n",
      "442, 0.4570, 0.1456, 0.7904, 0.6937, 30.1173\n",
      "443, 0.4520, 0.1454, 0.7934, 0.6937, 30.1860\n",
      "444, 0.4551, 0.1453, 0.7934, 0.7027, 30.2537\n",
      "445, 0.4521, 0.1451, 0.7924, 0.6847, 30.3216\n",
      "446, 0.4461, 0.1450, 0.7844, 0.7207, 30.3900\n",
      "447, 0.4464, 0.1448, 0.7824, 0.7207, 30.4585\n",
      "448, 0.4460, 0.1446, 0.7904, 0.7027, 30.5264\n",
      "449, 0.4418, 0.1445, 0.7984, 0.7027, 30.5943\n",
      "450, 0.4363, 0.1443, 0.7984, 0.7027, 30.6622\n",
      "451, 0.4317, 0.1442, 0.8024, 0.6937, 30.7301\n",
      "452, 0.4270, 0.1440, 0.8024, 0.7027, 30.7983\n",
      "453, 0.4222, 0.1438, 0.7954, 0.7207, 30.8658\n",
      "454, 0.4202, 0.1437, 0.7894, 0.7297, 30.9337\n",
      "455, 0.4203, 0.1435, 0.7884, 0.7297, 31.0022\n",
      "456, 0.4193, 0.1433, 0.7844, 0.7207, 31.0707\n",
      "457, 0.4168, 0.1432, 0.7874, 0.7207, 31.1387\n",
      "458, 0.4148, 0.1430, 0.7944, 0.7207, 31.2067\n",
      "459, 0.4132, 0.1428, 0.8014, 0.7207, 31.2747\n",
      "460, 0.4110, 0.1427, 0.7984, 0.7207, 31.3426\n",
      "461, 0.4088, 0.1425, 0.7954, 0.7027, 31.4108\n",
      "462, 0.4072, 0.1424, 0.7984, 0.7117, 31.4786\n",
      "463, 0.4060, 0.1422, 0.8044, 0.7117, 31.5465\n",
      "464, 0.4043, 0.1420, 0.8074, 0.7207, 31.6149\n",
      "465, 0.4026, 0.1419, 0.8094, 0.7117, 31.6829\n",
      "466, 0.4009, 0.1417, 0.8074, 0.7027, 31.7507\n",
      "467, 0.3991, 0.1415, 0.8074, 0.7027, 31.8191\n",
      "468, 0.3974, 0.1414, 0.8064, 0.7027, 31.8871\n",
      "469, 0.3956, 0.1412, 0.8074, 0.7027, 31.9550\n",
      "470, 0.3941, 0.1410, 0.8084, 0.7207, 32.0239\n",
      "471, 0.3925, 0.1409, 0.8134, 0.7207, 32.0919\n",
      "472, 0.3910, 0.1407, 0.8164, 0.7207, 32.1598\n",
      "473, 0.3897, 0.1406, 0.8174, 0.7117, 32.2282\n",
      "474, 0.3884, 0.1404, 0.8164, 0.7207, 32.2962\n",
      "475, 0.3869, 0.1402, 0.8164, 0.7207, 32.3642\n",
      "476, 0.3854, 0.1401, 0.8174, 0.7207, 32.4320\n",
      "477, 0.3840, 0.1399, 0.8184, 0.7117, 32.5003\n",
      "478, 0.3826, 0.1397, 0.8214, 0.7117, 32.5682\n",
      "479, 0.3812, 0.1396, 0.8234, 0.7117, 32.6361\n",
      "480, 0.3797, 0.1394, 0.8253, 0.7207, 32.7041\n",
      "481, 0.3782, 0.1393, 0.8234, 0.7207, 32.7723\n",
      "482, 0.3767, 0.1391, 0.8253, 0.7117, 32.8402\n",
      "483, 0.3753, 0.1389, 0.8253, 0.7117, 32.9082\n",
      "484, 0.3738, 0.1388, 0.8253, 0.7027, 32.9765\n",
      "485, 0.3724, 0.1386, 0.8273, 0.7117, 33.0444\n",
      "486, 0.3709, 0.1384, 0.8283, 0.7027, 33.1124\n",
      "487, 0.3695, 0.1383, 0.8323, 0.7027, 33.1804\n",
      "488, 0.3679, 0.1381, 0.8323, 0.7117, 33.2483\n",
      "489, 0.3663, 0.1380, 0.8333, 0.7117, 33.3162\n",
      "490, 0.3648, 0.1378, 0.8343, 0.7027, 33.3843\n",
      "491, 0.3631, 0.1376, 0.8353, 0.6937, 33.4522\n",
      "492, 0.3617, 0.1375, 0.8393, 0.6937, 33.5202\n",
      "493, 0.3602, 0.1373, 0.8383, 0.7027, 33.5881\n",
      "494, 0.3586, 0.1372, 0.8383, 0.7117, 33.6561\n",
      "495, 0.3571, 0.1370, 0.8393, 0.7117, 33.7241\n",
      "496, 0.3555, 0.1368, 0.8403, 0.7027, 33.7919\n",
      "497, 0.3540, 0.1367, 0.8443, 0.7027, 33.8604\n",
      "498, 0.3522, 0.1365, 0.8433, 0.6937, 33.9283\n",
      "499, 0.3506, 0.1364, 0.8423, 0.6937, 33.9964\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_accuracy_list = []\n",
    "\n",
    "directory = os.path.join(args.write_dir, args.dataset, str(args.num_clusters), str(args.idx), str(args.hidden_dim)+'_'+str(args.number_layers)+'_'+str(args.lr))\n",
    "print(args.dataset, str(args.num_clusters), str(args.idx), str(args.hidden_dim), str(args.number_layers), str(args.lr))\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "f = open(os.path.join(directory, 'log.txt'), 'a')\n",
    "if dataset_name in ['hiv', 'bace', 'bbbp', 'UPFD']:\n",
    "    print('Epoch, Loss, Reg Loss, Train rocauc, Val rocauc, Test rocauc, Time')\n",
    "elif dataset_name in ['MNIST', 'CIFAR10']:\n",
    "    print('Epoch, Loss, Reg Loss, Train Accuracy, Val Accuracy, Test Accuracy, Time')\n",
    "else:\n",
    "    print('Epoch, Loss, Reg Loss, Train Accuracy, Test Accuracy, Time')\n",
    "\n",
    "results = []\n",
    "for epoch in (range(500)):\n",
    "    loss_value_list, regularziation_value_list = train(batched_dataset_train_node, batched_dataset_train_edge, clustered_edge_index_train, clustered_batch_train, y_true_train)\n",
    "    y_pred_train = test(batched_dataset_train_node, batched_dataset_train_edge, clustered_edge_index_train, clustered_batch_train, y_true_train)\n",
    "    y_pred_test = test(batched_dataset_test_node, batched_dataset_test_edge, clustered_edge_index_test, clustered_batch_test, y_true_test)\n",
    "\n",
    "    loss_value = np.mean(loss_value_list)\n",
    "    regularziation_value = np.mean(regularziation_value_list)\n",
    "\n",
    "    # reshape y_true\n",
    "    y_true_train_1d = torch.cat(y_true_train).view(-1).numpy()\n",
    "    y_true_test_1d = torch.cat(y_true_test).view(-1).numpy()\n",
    "    y_true_train_2d = np.zeros((y_true_train_1d.size, y_true_train_1d.max()+1))\n",
    "    y_true_train_2d[np.arange(y_true_train_1d.size),y_true_train_1d] = 1\n",
    "    y_true_test_2d = np.zeros((y_true_test_1d.size, y_true_test_1d.max()+1))\n",
    "    y_true_test_2d[np.arange(y_true_test_1d.size),y_true_test_1d] = 1\n",
    "\n",
    "    if dataset_name in ['MNIST', 'CIFAR10', 'hiv', 'bace', 'bbbp', 'UPFD']:\n",
    "        y_pred_val = test(batched_dataset_val_node, batched_dataset_val_edge, clustered_edge_index_val, clustered_batch_val, y_true_val)\n",
    "        y_true_val_1d = torch.cat(y_true_val).view(-1).numpy()\n",
    "        y_true_val_2d = np.zeros((y_true_val_1d.size, y_true_val_1d.max()+1))\n",
    "        y_true_val_2d[np.arange(y_true_val_1d.size),y_true_val_1d] = 1\n",
    "\n",
    "    if dataset_name in ['hiv', 'bace', 'bbbp', 'UPFD']:\n",
    "        train_accuracy = roc_auc_score(y_true_train_2d, y_pred_train.numpy())\n",
    "        val_accuracy = roc_auc_score(y_true_val_2d, y_pred_val.numpy())\n",
    "        test_accuracy = roc_auc_score(y_true_test_2d, y_pred_test.numpy())\n",
    "        results.append([epoch, loss_value, regularziation_value, train_accuracy, val_accuracy, test_accuracy, time.time()-start_time])\n",
    "        print(\"{:03d}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(epoch, loss_value, regularziation_value, train_accuracy, val_accuracy, test_accuracy, time.time()-start_time))\n",
    "    elif dataset_name in ['MNIST', 'CIFAR10']:\n",
    "        y_pred_train, y_pred_val, y_pred_test = y_pred_train.argmax(dim=1), y_pred_val.argmax(dim=1), y_pred_test.argmax(dim=1)\n",
    "        train_accuracy = accuracy_score(y_true_train_1d, y_pred_train.numpy())\n",
    "        val_accuracy = accuracy_score(y_true_val_1d, y_pred_val.numpy())\n",
    "        test_accuracy = accuracy_score(y_true_test_1d, y_pred_test.numpy())\n",
    "        results.append([epoch, loss_value, regularziation_value, train_accuracy, val_accuracy, test_accuracy, time.time()-start_time])\n",
    "        print(\"{:03d}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(epoch, loss_value, regularziation_value, train_accuracy, val_accuracy, test_accuracy, time.time()-start_time))\n",
    "    else:\n",
    "        y_pred_train, y_pred_test = y_pred_train.argmax(dim=1), y_pred_test.argmax(dim=1)\n",
    "        train_accuracy = accuracy_score(y_true_train_1d, y_pred_train.numpy())\n",
    "        test_accuracy = accuracy_score(y_true_test_1d, y_pred_test.numpy())\n",
    "        results.append([epoch, loss_value, regularziation_value, train_accuracy, test_accuracy, time.time()-start_time])\n",
    "        print(\"{:03d}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(epoch, loss_value, regularziation_value, train_accuracy, test_accuracy, time.time()-start_time))\n",
    "\n",
    "if dataset_name in ['hiv', 'bace', 'bbbp', 'UPFD']:\n",
    "    df = pd.DataFrame(data=results, columns=['Epoch', 'Loss', 'Reg Loss', 'Train rocauc', 'Val rocauc', 'Test rocauc', 'Time'])\n",
    "elif dataset_name in ['MNIST', 'CIFAR10']:\n",
    "    df = pd.DataFrame(data=results, columns=['Epoch', 'Loss', 'Reg Loss', 'Train Accuracy', 'Val Accuracy', 'Test Accuracy', 'Time'])\n",
    "else:\n",
    "    df = pd.DataFrame(data=results, columns=['Epoch', 'Loss', 'Reg Loss', 'Train Accuracy', 'Test Accuracy', 'Time'])\n",
    "\n",
    "#df.to_csv(os.path.join(directory, 'results.csv'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f4d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c51c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cb13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
